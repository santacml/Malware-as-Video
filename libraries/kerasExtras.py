import gzip, numpy, pickle
import keras
from keras import backend as K
from keras import optimizers
import numpy as np
from random import shuffle

import matplotlib
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

window_size = 25
# window_size = 50


def reshape_operator( x4d ):
    import tensorflow as tf
    
    window_tensor = tf.constant(window_size)
    shape = tf.shape( x4d )
    new_d2 = tf.cast(shape[1]/window_tensor, dtype=tf.int32)
    x5d = tf.reshape( x4d, [shape[0] , new_d2, window_tensor, shape[2], shape[3]] )
    return x5d

def reshape_shape( x4d_shape ):
    import tensorflow as tf
    
    in_batch, in_rows, in_cols, in_filters = x4d_shape
    output_shape = ( in_batch, None, window_size, in_cols, in_filters)
    return output_shape
    
    
def reshape_operator_noembed( x3d ):
    import tensorflow as tf
    
    window_tensor = tf.constant(window_size)
    shape = tf.shape( x3d )
    new_d2 = tf.cast(shape[1]/window_tensor, dtype=tf.int32)
    x4d = tf.reshape( x3d, [shape[0] , new_d2, window_tensor, shape[2], 1] )
    return x4d

def reshape_shape_noembed( x3d_shape ):
    import tensorflow as tf
    
    in_batch, in_rows, in_cols = x3d_shape
    output_shape = ( in_batch, None, window_size, in_cols, 1)
    return output_shape

def mean_operator(x5d):
    return K.mean(x5d, axis=1, keepdims=False)
    
def mean_shape(x5d_shape):
    in_batch, in_steps, in_window_size, in_cols, in_filters = x5d_shape
    output_shape = (in_batch, in_window_size, in_cols, in_filters)
    return output_shape
    
def max_operator(x5d):
    return K.max(x5d, axis=1, keepdims=False)
    
def max_shape(x5d_shape):
    in_batch, in_steps, in_window_size, in_cols, in_filters = x5d_shape
    output_shape = (in_batch, in_window_size, in_cols, in_filters)
    return output_shape

def max_min_mean_operator(x5d):
    max = K.max(x5d, axis=1, keepdims=True)
    min = K.min(x5d, axis=1, keepdims=True)
    mean = K.mean(x5d, axis=1, keepdims=True)
    return K.concatenate([max, min, mean], axis=1)
    
def max_min_mean_shape(x5d_shape):
    in_batch, in_steps, in_window_size, in_cols, in_filters = x5d_shape
    output_shape = (in_batch, 3, in_window_size, in_cols, in_filters)
    return output_shape
    
    
    
    
# batch_size = 5
batch_size = 1

def loadDataGenerator(assemblyVectorizedFile, maxNumSamples=10000, divideBy255=False, randLineOrder=False, randWindowOrder=False):
    readMe = gzip.open(assemblyVectorizedFile, "rb")
    numSamples = 0
    
    while True:
        try:
            assert numSamples < maxNumSamples
            
            xArr = []
            answers = []
            # for x in range(0,batch_size):
            fileArr = pickle.load(readMe)
            xArr.append(fileArr[0])
            answers.append(fileArr[1])
            
            train_x = numpy.asarray(xArr, dtype="float32") 
            if divideBy255: 
                train_x /= 255
            
            if randLineOrder:
                np.random.shuffle(train_x[0])
            if randWindowOrder:
                shape = train_x.shape
                tmp = np.reshape(train_x, (shape[0], int(shape[1]/window_size), window_size, shape[2]))
                np.random.shuffle(train_x[0])
                train_x = np.reshape(tmp, shape)
            
            ans = []
            # for x in range(0,batch_size):
            num = answers.pop(0)[0]
            answer = [0]*9
            answer[num-1] = 1
            ans.append(answer)
            # print("ANSWER IS", num)
            train_y = numpy.asarray(ans, dtype="float32")
            
            numSamples += 1
            
            yield (train_x, train_y)
                
            
            
            
        except (EOFError, AssertionError) as e :
            del readMe
            readMe = gzip.open(assemblyVectorizedFile, "rb")
            numSamples = 0
    
def loadDataGeneratorBinary(assemblyVectorizedFile, maxNumSamples=10000):
    readMe = gzip.open(assemblyVectorizedFile, "rb")
    numSamples = 0
    
    while True:
        try:
            assert numSamples < maxNumSamples
            
            xArr = []
            answers = []
            # for x in range(0,batch_size):
            fileArr = pickle.load(readMe)
            xArr.append(fileArr[0])
            answers.append(fileArr[1])
            
            train_x = numpy.asarray(xArr, dtype="float32") 
            # print( train_x.shape[1])
            # if train_x.shape[1] > 150000: 
            if train_x.shape[1] > 100000: 
                # print("skipping", train_x.shape[1])
                continue
            
            # disarrange(train_x, axis=1)
            
            # ans = []
            # for x in range(0,batch_size):
                # num = answers.pop(0)[0]
                # answer = [0]*9
                # answer[num-1] = 1
                # ans.append(answer)
                # print("ANSWER IS", num)
            train_y = numpy.asarray(answers, dtype="float32")
            
            numSamples += 1
            
            yield (train_x, train_y)
                
            
            
            
        except (EOFError, AssertionError) as e :
            del readMe
            readMe = gzip.open(assemblyVectorizedFile, "rb")
            numSamples = 0
    
def loadDataGeneratorPartial(assemblyVectorizedFile, maxNumSamples=10000, percentOfFile=1, loc=0):
    if percentOfFile > 1: 
        print("percentOfFile cannot be > 1")
        print(0/0)
    
    readMe = gzip.open(assemblyVectorizedFile, "rb")
    numSamples = 0
    
    while True:
        try:
            assert numSamples < maxNumSamples
            
            xArr = []
            answers = []
            # for x in range(0,batch_size):
            fileArr = pickle.load(readMe)
            xArr.append(fileArr[0])
            answers.append(fileArr[1])
            
            train_x = numpy.asarray(xArr, dtype="float32") 
            # print(train_x.shape)
            x_len = percentOfFile*train_x.shape[1]
            base = 25
            newEnd = int(base * round(float(x_len)/base))
            
            # if newEnd == 0:
                # yield (False, False)
                # newEnd = 25
                # continue
            
            # print(newEnd)
            if loc == 0:
                train_x = train_x[:, :newEnd]
            elif loc == 1:
                train_x = train_x[:, -newEnd:]
            else:
                print("what percent of file")
            # print(train_x.shape)
            
            # disarrange(train_x, axis=1)
            
            ans = []
            for x in range(0,batch_size):
                num = answers.pop(0)[0]
                answer = [0]*9
                answer[num-1] = 1
                ans.append(answer)
                # print("ANSWER IS", num)
            train_y = numpy.asarray(ans, dtype="float32")
            
            numSamples += 1
            
            yield (train_x, train_y)
                
            
            
            
        except (EOFError, AssertionError) as e :
            del readMe
            readMe = gzip.open(assemblyVectorizedFile, "rb")
            numSamples = 0
    
def loadDataGeneratorAllClasses(assemblyVectorizedFile, maxNumSamples=10000):
    readMe = gzip.open(assemblyVectorizedFile, "rb")
    numSamples = 0
    
    while True:
        try:
            assert numSamples < maxNumSamples
            
            xArr = []
            answers = []
            # for x in range(0,batch_size):
            fileArr = pickle.load(readMe)
            xArr.append(fileArr[0])
            answers.append(fileArr[1])
            # print(answers)
            
            train_x = numpy.asarray(xArr, dtype="float32") 
            
            # disarrange(train_x, axis=1)
            
            ans = []
            for x in range(0,batch_size):
                num = answers.pop(0)
                answer = [0]*11
                answer[num-1] = 1
                ans.append(answer)
                # print("ANSWER IS", num)
            train_y = numpy.asarray(ans, dtype="float32")
            
            numSamples += 1
            
            yield (train_x, train_y)
                
            
            
            
        except (EOFError, AssertionError) as e :
            del readMe
            readMe = gzip.open(assemblyVectorizedFile, "rb")
            numSamples = 0
    
def loadDataGeneratorBatchesShuffle(assemblyVectorizedFile, maxNumSamples=10000):
    readMe = gzip.open(assemblyVectorizedFile, "rb")
    numSamples = 0
    
    while True:
        try:
            samples = []
            for n in range(0, 500):
                assert numSamples < maxNumSamples
                
                xArr = []
                answers = []
                # for x in range(0,batch_size):
                fileArr = pickle.load(readMe)
                xArr.append(fileArr[0])
                answers.append(fileArr[1])
                
                train_x = numpy.asarray(xArr, dtype="float32") 
                
                # disarrange(train_x, axis=1)
                
                ans = []
                # for x in range(0,batch_size):
                num = answers.pop(0)[0]
                answer = [0]*9
                answer[num-1] = 1
                ans.append(answer)
                # print("ANSWER IS", num)
                train_y = numpy.asarray(ans, dtype="float32")
                
                numSamples += 1
                
                # yield (train_x, train_y)
                
                samples.append((train_x, train_y))
            
            shuffle(samples)
            
            for n in range(0, 50):
                yield samples[n]
            
            
        except (EOFError, AssertionError) as e :
            del readMe
            readMe = gzip.open(assemblyVectorizedFile, "rb")
            numSamples = 0
            
            
            
            
def loadAllAndShuffle(trainFile, testFile):
    # take a train and validation file and return as one big, shuffled dataset
    # because that's how it's divided when I make it
    
    files = [trainFile, testFile]
    
    numSamples = 0
    samples = []
    for file in files:
        readMe = gzip.open(file, "rb")
        done = False
        while not done:
            try:
                xArr = []
                answers = []
                # for x in range(0,batch_size):
                fileArr = pickle.load(readMe)
                xArr.append(fileArr[0])
                answers.append(fileArr[1])
                
                train_x = numpy.asarray(xArr, dtype="float32") 
                
                
                ans = []
                # for x in range(0,batch_size):
                num = answers.pop(0)[0]
                answer = [0]*9
                answer[num-1] = 1
                ans.append(answer)
                # print("ANSWER IS", num)
                train_y = numpy.asarray(ans, dtype="float32")
                
                
                samples.append((train_x, train_y))
                numSamples += 1
                print("loaded", numSamples, "samples")
                
            except (EOFError, AssertionError) as e :
                done = True
        
        
    shuffle(samples)
    shuffle(samples)
    return samples
        
def loadDataGeneratorKFold(allSamples, k=10):
    # all samples from loadAllAndShuffle
    numFiles = len(allSamples)
    foldSize = int(numFiles / k)
    
    print("num files", numFiles)
    print("fold size", foldSize)
    
    folds = [allSamples[foldSize*n:foldSize*(n+1)] for n in range(0, k)]
    
    return folds
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    