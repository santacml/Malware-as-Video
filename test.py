import pickle, gzip, glob, sys, keras, os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # gets rid of AVX message
import random as rn
import numpy as np
import tensorflow as tf
os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(37)
rn.seed(1254)
tf.set_random_seed(89)
from keras import optimizers
from keras import backend as K
from keras.models import load_model
from keras.layers import *
from keras.models import Sequential
from keras.losses import weighted_categorical_crossentropy
from keras.callbacks import CSVLogger, ModelCheckpoint
from keras.regularizers import *
from keras.utils.generic_utils import get_custom_objects
from keras.layers.advanced_activations import LeakyReLU, ELU
sys.path.insert(0, r'.\libraries')
from kerasLayers import *
from kerasExtras import *
elu = ELU(1)
elu.__name__ = "ELU"

from kerassurgeon import identify
from kerassurgeon.operations import delete_channels
import time
from keras import Model
import matplotlib.pyplot as plt
import csv
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import numpy as np
import jenkspy


input_length = None


# line_length = 25
# num_samples = 8000
# num_samples_valid = 2375
# train_name = r".\libraries\datasets\kaggle\trainASM_all_nopad_pooled.pklz"
# valid_name = r".\libraries\datasets\kaggle\validASM_all_nopad_pooled.pklz"



# line_length = 18
# num_samples = 4000
# num_samples_valid = 1000
# train_name = r".\libraries\datasets\kagglewindows\no50k\windows_exe_dll_kaggle_no50k_pooled.pklz"
# valid_name = r".\libraries\datasets\kagglewindows\no50k\windows_exe_dll_kaggle_validation_no50k_pooled.pklz"


# line_length = 18
# num_samples = 6500
# num_samples_valid = 2050
# train_name = r".\libraries\datasets\kaggle\no50k\trainASM_no50k_pooled.pklz"
# valid_name = r".\libraries\datasets\kaggle\no50k\validASM_no50k_pooled.pklz"


# line_length = 25
# num_samples = 7500
# train_name = r".\libraries\datasets\kagglewindows0day\winkaggle_noclass8_nopad_pooled.pklz"
# num_samples_valid = 522   # experimentally for 0 day
# valid_name = r"D:\Research\1_Libraries\datasets\kagglewindows0day\winkaggle_onlyclass8_nopad_pooled.pklz"
# real values
# num_samples_valid = 9100 - 7500
# valid_name = r".\libraries\datasets\kagglewindows0day\winkaggle_noclass8_validation_nopad_pooled.pklz"



# binary standard
line_length = 25
num_samples = 7500
num_samples_valid = 1600
train_name = r".\libraries\datasets\kagglewindows\windows_exe_dll_kaggle_nopad_pooled.pklz"
valid_name = r".\libraries\datasets\kagglewindows\windows_exe_dll_kaggle_validation_nopad_pooled.pklz"




steps_per_epoch = num_samples/batch_size
valid_steps = num_samples_valid/batch_size  # should be this


# model = load_model(r".\networks\rnn binary final nets\addconv\KaggleConv-09.hdf5",
# model = load_model(r".\networks\rnn binary final nets\convlstm\KaggleConv-11.hdf5",
# model = load_model(r".\networks\rnn final nets\final addconv from 4\KaggleConv-27.hdf5",
# model = load_model(r".\networks\rnn final nets\final convlstm\KaggleConv-12.hdf5",
# model = load_model(r".\networks\dist binary final nets\0day networks\KaggleConv-15.hdf5",
# model = load_model(r".\networks\dist binary final nets\noembed\KaggleConv-42.hdf5",
# model = load_model(r".\networks\dist final nets\tanh from 63\KaggleConv-34.hdf5",
# model = load_model(r".\networks\dist final nets\pruned from 79\KaggleConv-88.hdf5",
model = load_model(r".\networks\dist binary final nets\pruned from 5\KaggleConv-22.hdf5",
                   custom_objects={'DecayingConvLSTM2D':MinConvRNN,
                                   'window_size': window_size ,
                                   'ELU': elu,
                                   }
                   )

# mode = "kaggle"
mode = "binary"
if mode == "binary":
    lossFunc = 'binary_crossentropy'
    generatorFunc = loadDataGeneratorBinary
elif mode == "kaggle":
    lossFunc = 'categorical_crossentropy'
    generatorFunc = loadDataGenerator


TESTS = [
    "confMatrix",
    # "quantize",
    # "partialFile",
    # "prune",
]


partialFileResultsFile = r"Partial_File_Results.csv"


def compileModel(model):
    optimizer = "rmsprop"
    
    model.compile(optimizer=optimizer,
                  loss=lossFunc,
                  metrics=['accuracy'])

def basicTest(model, confMatrixFile):
    if mode == "binary" or mode == "kaggle":
        train_gen = generatorFunc(train_name, num_samples)
        valid_gen = generatorFunc(valid_name, num_samples_valid)
    else: 
        raise ValueError("mode must be kaggle or binary")
    
    compileModel(model)
    print(model.summary())

    lengths = []
    correctLengths = []
    incorrectLengths = []
    answers = []
    preds = []
    total_correct = 0
    total_run = 0
    correctCountClasses = [0]*9
    incorrectCountClasses = [0]*9
    times = []
    confMatrix = np.zeros((9,9))

    for x in range(0, int(valid_steps)):
        train_x, train_y = next(valid_gen)
    # for x in range(0, int(steps_per_epoch)):
        # train_x, train_y = next(train_gen)
        length = train_x.shape[1]
        lengths.append(length)
        answer = train_y.tolist()[0]
        answers.append(answer)
        
        
        start = time.time()
        pred = model.predict(train_x).tolist()[0]
        amt = time.time() - start
        times.append(amt)
        preds.append(pred)
        
        # print(train_x[0][:10])
        # print()
        # print()
        # from keras import Model
        # intermediate_layer_model = Model(inputs=model.input,
                                     # outputs=model.get_layer("max_pooling1d_1").output)
                                     # outputs=model.get_layer("global_average_pooling2d_1").output)
                                     # outputs=model.get_layer("dense_1").output)
        # intermediate_output = intermediate_layer_model.predict(train_x)
        
        # img = intermediate_output[0]
        # img = img.reshape((5,10))
        # ansClass = answer.index(max(answer))
        # predClass = pred.index(max(pred)) 
        # if ansClass == 3:
            # print(ansClass, predClass)
            # import matplotlib.pyplot as plt
            # plt.imshow(img, cmap='gray')
            # plt.show()
        # else:
            # continue
            
        # session=K.get_session()
        # grads = session.run(tf.gradients(model.output, model.input), feed_dict={model.input:train_x})
        
        
        ''' original code for saliency test - can remove
        final_tensor = tf.Variable(train_y)
        loss = K.categorical_crossentropy(final_tensor, model.output)
        grad = K.gradients(model.layers[-1].input, model.layers[1].output)[0]
        sess = K.get_session()
        out = sess.run(grad, feed_dict={model.input: train_x})
        out = out[0] # first frame   1, 10, 25, 25, 100
        out = out.reshape((int(out.shape[0]*out.shape[1]), out.shape[2], out.shape[3]))
        print(train_y)
        fig = plt.figure()
        ax1 = fig.add_subplot(1,4,1)
        ax1.imshow(train_x[0], cmap='gray')
        ax2 = fig.add_subplot(1,4, 2)
        ax2.imshow(np.mean(out, axis=2), cmap='gray')
        ax3 = fig.add_subplot(1,4, 3)
        ax3.imshow(np.max(out, axis=2), cmap='gray')
        ax4 = fig.add_subplot(1,4, 4)
        ax4.imshow(np.min(out, axis=2), cmap='gray')
        plt.show()
        # plt.imshow(out)
        # print(0/0)
        '''

        
        if mode == "binary":
        
            pred = round(pred[0])  
            
            
            ansClass = int(answer)
            
            if pred == ansClass:
                total_correct += 1
                correctCountClasses[ansClass] += 1
            else:
                incorrectCountClasses[ansClass] += 1
                
            confMatrix[ansClass][pred] += 1
            total_run += 1
            if x % 500 == 0: print("interval", x, "correct so far", total_correct)
        
        elif mode == "kaggle":
            # ansClass = numpy.argmax(train_y,1)[0]
            ansClass = answer.index(max(answer))
            predClass = pred.index(max(pred)) 
            
            if predClass == ansClass:
                total_correct += 1
                correctLengths.append(length)
                correctCountClasses[ansClass] += 1
            else:
                incorrectLengths.append(length)
                incorrectCountClasses[ansClass] += 1
                
            confMatrix[ansClass][predClass] += 1
            total_run += 1
            if x % 500 == 0: print("interval", x, "correct so far", total_correct)

    print("correct:", total_correct, "out of", total_run)
    print("correct per class  :", correctCountClasses)
    print("incorrect per class:", incorrectCountClasses)
    print("mean time to predict", np.mean(times))
    
    
    print(confMatrix)
    if confMatrixFile:
        numpy.savetxt(confMatrixFile, confMatrix, delimiter=",")

if __name__ == "__main__":
    if "confMatrix" in TESTS:
        print("--------------------Performing Confusion Matrix Test--------------------")
        basicTest(model, "confMatrix.csv")

    if "partialFile" in TESTS:
        if mode == "binary":
            print("Partial file only implemented for malware dataset.")
            print("To run on binary, implement the loadDataGeneratorPartial for the malware vs. benign dataset.") 
            0/0
            
        print("--------------------Performing Partial File Test--------------------")
        compileModel(model)
        print(model.summary())


        with open(partialFileResultsFile, mode='w', newline='') as results_file:
            results_writer = csv.writer(results_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)

            
            results_writer.writerow(['Percent of file', 'Train set accuracy', 'Validation set accuracy'])
            
            
            for percentOfFile in np.arange(0.01,1.01,.01):
                # results_writer.writerow(['Percent of file', 'Train set accuracy', 'Validation set accuracy'])
                # continue
                
                train_gen = loadDataGeneratorPartial(train_name, maxNumSamples=num_samples, percentOfFile=percentOfFile, loc=0)
                valid_gen = loadDataGeneratorPartial(valid_name, maxNumSamples=num_samples_valid, percentOfFile=percentOfFile, loc=0)
                
                
                
                answers = []
                preds = []
                total_correct = 0
                total_run = 0
                correctCountClasses = [0]*9
                incorrectCountClasses = [0]*9

                for x in range(0, int(valid_steps)):
                    train_x, train_y = next(valid_gen)
                    # print(train_x.shape)
                    if train_x.shape[1] == 0:
                        continue
                    
                    answer = train_y.tolist()[0]
                    
                    pred = model.predict(train_x).tolist()[0]
                    
                    
                    ansClass = numpy.argmax(train_y,1)[0]
                    
                    if pred.index(max(pred)) == answer.index(max(answer)):
                        total_correct += 1
                        correctCountClasses[ansClass] += 1
                    else:
                        incorrectCountClasses[ansClass] += 1
                        
                    total_run += 1
                    # if x % 500 == 0: print("interval", x, "correct so far", total_correct)

                # print("correct:", total_correct, "out of", total_run)
                # print("correct per class  :", correctCountClasses)
                # print("incorrect per class:", incorrectCountClasses)
                
                validAcc = total_correct / total_run
                
                
                answers = []
                preds = []
                total_correct = 0
                total_run = 0
                correctCountClasses = [0]*9
                incorrectCountClasses = [0]*9

                for x in range(0, int(num_samples)):
                    train_x, train_y = next(train_gen)
                    
                    if train_x.shape[1] == 0:
                        continue
                    
                    answer = train_y.tolist()[0]
                    
                    pred = model.predict(train_x).tolist()[0]
                    
                    
                    ansClass = numpy.argmax(train_y,1)[0]
                    
                    if pred.index(max(pred)) == answer.index(max(answer)):
                        total_correct += 1
                        correctCountClasses[ansClass] += 1
                    else:
                        incorrectCountClasses[ansClass] += 1
                        
                    total_run += 1
                    # if x % 500 == 0: print("interval", x, "correct so far", total_correct)

                # print("correct:", total_correct, "out of", total_run)
                # print("correct per class  :", correctCountClasses)
                # print("incorrect per class:", incorrectCountClasses)
                
                trainAcc = total_correct / total_run
                
                
                results_writer.writerow([percentOfFile, trainAcc, validAcc])
                
                print(percentOfFile, trainAcc, validAcc)
        
    if "quantize" in TESTS:
        print("--------------------Performing Quantization Test--------------------")
        train_gen = generatorFunc(tain_name, num_samples)
        valid_gen = generatorFunc(valid_name, num_samples_valid)
        
        


        def find_nearest(array, value):
            array = np.asarray(array)
            idx = (np.abs(array - value)).argmin()
            return array[idx]


        allVals = []
        for layer in model.layers:
            weights = layer.get_weights()
            
            if not weights: continue
            
            newKernels = []
            
            
            pos = -1
            for kernel in weights:
                pos += 1
                if layer == model.layers[-1] and pos == 1:
                    newKernels.append(kernel)
                    continue
                # kernel = weights[0]
                
                x = np.sort(kernel.flatten())
                
                replaceDict = {}
                uniqueItems = len(set(kernel.flatten().tolist()))
                itemcnt = int(4*math.log(uniqueItems))
                
                
                # '''
                breaks = jenkspy.jenks_breaks(x, nb_class=itemcnt)
                breaks = np.asarray(breaks)
                for n in range(0, x.shape[0]):
                    distances = np.abs(breaks - x[n])
                    val = breaks[np.argmin(distances)]
                    replaceDict[x[n]] = val
                # '''
                
                
                newKernel = kernel.flatten().tolist()
                for i in range(0, len(newKernel)):
                    newKernel[i] = replaceDict[newKernel[i]]
                    
                allVals.extend(newKernel)
                
                
                # numpy.savetxt("origKernel.csv", np.sort(kernel.flatten()), delimiter=",")
                # numpy.savetxt("newKernel.csv", np.sort(newKernel), delimiter=",")
                # 0/0
                
                
                newKernel = np.asarray(newKernel)
                newKernel = np.reshape(newKernel, kernel.shape)
                
                
                print("layer", layer)
                print("Kernel unique items:", uniqueItems)
                print("New Kernel unique items:", len(set(newKernel.flatten().tolist())))
                
                
                # code to show new kernel values
                
                # xAxis = np.arange(0, axis)
                # plt.plot(xAxis,sorted(layer.get_weights()[0].flatten()))
                # plt.plot(xAxis,sorted(newKernel.flatten()))
                # plt.title("asdf")
                # plt.show()
                # print(0/0)
                
                newKernels.append(newKernel)
            
            
            layer.set_weights(newKernels)
            

        print("--------------------compiling and testing new model--------------------")
        compileModel(model)
                      
        # print(model.summary())
                      
        # model.save(r'newmodel.hdf5')
        
        
        basicTest(model, None)

    if "prune" in TESTS:
        print("--------------------Pruning Network--------------------")
        train_gen = generatorFunc(train_name, num_samples)
        valid_gen = generatorFunc(valid_name, num_samples_valid)
        
        compileModel(model)
        print(model.summary())



        for x in range(1,3):
            # '''
                               
            # layer_name = 'time_distributed_4'
            layer_name = 'time_distributed_2'
            # layer_name = 'time_distributed_1'
            # cnt = 5
            cnt = 2
            
            
            layer = model.get_layer(name=layer_name)
            
            '''  old pruning attempts - just keeping for posterity. Couldn't put everything on github due to PA approval.

            # apoz = identify.get_apoz(model, layer, valid_gen, steps=num_samples_valid_for_ID)
            # channels_to_prune = identify.high_apoz(apoz, "both")
            
            # apoz = identify.get_sum(model, layer, valid_gen, steps=num_samples_valid_for_ID)
            # apoz = identify.get_lowest_mag_kernels(layer)
            
            # channels_to_prune = identify.lowest_cnt(apoz, cnt)
            
            
            # channels_to_prune = identify.get_kernels(layer, cnt)
            
            
            # channels_to_prune = identify.get_output_dist(model, layer, valid_gen, steps=num_samples_valid_for_ID, pruneCnt=cnt)
            
            # channels_to_prune = identify.get_output_dist_adj_matrix_embed(model, layer, valid_gen, steps=num_samples_valid_for_ID, pruneCnt=cnt)
            
            # channels_to_prune = identify.get_output_dist_adj_matrix(model, layer, valid_gen, steps=num_samples_valid, pruneCnt=cnt)
            # print("to prune", channels_to_prune)
            # valid_gen = loadDataGenerator(r".\kaggle_dataset\vectorized\validASM_all_nopad_pooled.pklz", num_samples_valid)
            
            channels_to_prune = identify.get_output_dist_adj_matrix_UPDATE(model, layer, valid_gen, steps=num_samples_valid, pruneCnt=cnt)
            print("to prune", channels_to_prune)
            
            
            # distances = identify.get_output_dist_adj_matrix_RETURN_SORTED_DISTANCES(model, layer, valid_gen, steps=num_samples_valid, pruneCnt=cnt)
            # numpy.savetxt("node_distances.csv", np.asarray(distances), delimiter=",")
            # print(distances)
            # 0/0
            '''
            channels_to_prune = identify.get_nodedist(model, layer, valid_gen, steps=num_samples_valid, pruneCnt=cnt)
            print("to prune", channels_to_prune)
            
            model = delete_channels(model, layer, channels_to_prune)
            

            model.compile(optimizer='rmsprop',
                          loss='categorical_crossentropy',
                          metrics=['accuracy'])
            print("Pruned summary")
            print(model.summary())
            
            csv_logger = CSVLogger(r'.\networks\KaggleTrainingSeqConvPrune' + str(x) + '.log')
            filepath = r".\networks\KaggleConvPrune" + str(x) + "-{epoch:02d}.hdf5"
            checkpoint = ModelCheckpoint(filepath)

            model.fit_generator(train_gen,
                                epochs=20,
                                callbacks=[csv_logger, checkpoint],
                                steps_per_epoch=num_samples,
                                validation_data=valid_gen,
                                validation_steps=num_samples_valid,
            )
    
    print("--------------------Testing Over--------------------")